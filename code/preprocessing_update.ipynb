{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Conversion C3D en JSON rééchantilloné à 50 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ezc3d\n",
    "import json\n",
    "from scipy.signal import resample,butter, lfilter, freqz\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_evenements(file_path):\n",
    "    \"\"\"\n",
    "    Cette fonction extrait les événements du fichier C3D.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Le chemin du fichier C3D.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les événements et leur temps correspondant.\n",
    "    \"\"\"\n",
    "    c3d = ezc3d.c3d(file_path)\n",
    "    events = c3d['parameters']['EVENT']['LABELS']['value'] # Récupérer les événements\n",
    "    temps_events = c3d['parameters']['EVENT']['TIMES']['value'] # Récupérer les temps correspondant aux événements\n",
    "    # Créer un DataFrame\n",
    "    df = pd.DataFrame({'events': events, 'frames': temps_events[1]})\n",
    "    # Supprimer l'index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    events_dict = df.groupby('events')['frames'].apply(list).to_dict() #self.events final\n",
    "    return events_dict\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=2):\n",
    "    \"\"\"\n",
    "    Crée les coefficients d'un filtre Butterworth passe-bas.\n",
    "\n",
    "    Args:\n",
    "        cutoff (float): Fréquence de coupure du filtre.\n",
    "        fs (float): Fréquence d'échantillonnage du signal.\n",
    "        order (int): Ordre du filtre.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Coefficients du filtre (b, a).\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs # Fréquence de Nyquist\n",
    "    normal_cutoff = cutoff / nyq # Fréquence de coupure normalisée\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False) # Création des coefficients du filtre\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    \"\"\"\n",
    "    Applique un filtre Butterworth passe-bas à un signal.\n",
    "\n",
    "    Args:\n",
    "        data (array_like): Signal à filtrer.\n",
    "        cutoff (float): Fréquence de coupure du filtre.\n",
    "        fs (float): Fréquence d'échantillonnage du signal.\n",
    "        order (int): Ordre du filtre.\n",
    "\n",
    "    Returns:\n",
    "        array_like: Signal filtré.\n",
    "    \"\"\"\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def reechantillonnage_fc_coupure_et_association_labels_et_data(file_path, cutoff_freq=20, target_freq=50):\n",
    "    \"\"\"\n",
    "    Cette fonction transforme les données échantillonnées à 2100 Hz en données échantillonnées à 50 Hz,\n",
    "    puis applique un filtre Butterworth passe-bas avec une fréquence de coupure de 20 Hz, et enfin associe les étiquettes des données aux données rééchantillonnées et filtrées.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Le chemin du fichier C3D.\n",
    "        cutoff_freq (float): Fréquence de coupure du filtre Butterworth (20 Hz).\n",
    "        target_freq (float): Fréquence cible après rééchantillonnage (50 Hz).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Un tuple contenant un dictionnaire fusionnant les étiquettes et les données rééchantillonnées et filtrées, et les temps rééchantillonnés.\n",
    "    \"\"\"\n",
    "    c3d = ezc3d.c3d(file_path)\n",
    "    labels = c3d['parameters']['ANALOG']['LABELS']['value'] # Récupérer les étiquettes des données\n",
    "    original_freq = c3d['parameters']['ANALOG']['RATE']['value'][0] # Récupérer la fréquence d'échantillonnage\n",
    "    data = c3d['data']['analogs'] # Récupérer les données\n",
    "    nb_frame = len(data[0][0]) # Récupérer le nombre de frames\n",
    "    target_freq = 50  # fréquence de rééchantillonnage\n",
    "    \n",
    "    # Rééchantillonnage des données\n",
    "    nb_samples_target = int(nb_frame * (target_freq / original_freq)) # Calcul du nombre d'échantillons cible\n",
    "    resampled_times = np.linspace(0., (nb_frame / original_freq), num=nb_samples_target) # Création d'un tableau de temps rééchantillonné\n",
    "    resampled_data = np.zeros((len(labels), nb_samples_target)) # Création d'un tableau de zéros de la taille des données rééchantillonnées\n",
    "    for i in range(len(labels)): # Pour chaque étiquette\n",
    "        resampled_data[i, :] = resample(data[0][i, :], nb_samples_target) # Rééchantillonnage des données\n",
    "\n",
    "    fusion_label_data = {}\n",
    "    for i, label in enumerate(labels): # Pour chaque étiquette\n",
    "        fusion_label_data[label] = resampled_data[i, :] # Associer les étiquettes aux données rééchantillonnées\n",
    "        \n",
    "    filtered_fusion_label_data = {}  # Initialiser un dictionnaire pour stocker les données filtrées\n",
    "\n",
    "    for label, data in fusion_label_data.items():  # Itérer sur chaque étiquette et signal\n",
    "        filtered_signal = butter_lowpass_filter(data, cutoff_freq, target_freq)  # Appliquer le filtre Butterworth\n",
    "        filtered_fusion_label_data[label] = filtered_signal  # Stocker le signal filtré dans le dictionnaire de données filtrées\n",
    "    \n",
    "    return filtered_fusion_label_data, resampled_times\n",
    "    \n",
    "\n",
    "def filtrer_labels(filtered_fusion_label_data):\n",
    "    \"\"\"\n",
    "    Cette fonction filtre les étiquettes des données pour ne garder que celles liées à l'accéléromètre et au gyroscope.\n",
    "\n",
    "    Args:\n",
    "        fusion_label_data (dict): Dictionnaire contenant les étiquettes et les données associées.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Un tuple contenant une liste des étiquettes filtrées et un dictionnaire des données associées aux étiquettes filtrées.\n",
    "    \"\"\"\n",
    "    labels_filtre = []\n",
    "    labels_data_filtre = {}\n",
    "    for label, valeurs in filtered_fusion_label_data.items(): # Pour chaque étiquette et ses valeurs\n",
    "        if 'ACC' in label or 'GYRO' in label: # Si l'étiquette contient 'ACC' ou 'GYRO'\n",
    "            labels_filtre.append(label) # Ajouter l'étiquette à la liste des étiquettes filtrées\n",
    "            labels_data_filtre[label] = valeurs # Ajouter les valeurs associées à l'étiquette dans le dictionnaire des données filtrées\n",
    "    return labels_filtre, labels_data_filtre\n",
    "\n",
    "def calcul_norme(labels_data_filtre):\n",
    "    \"\"\"\n",
    "    Cette fonction calcule les normes des données filtrées.\n",
    "\n",
    "    Args:\n",
    "        labels_data_filtre (dict): Dictionnaire contenant les étiquettes et les données associées, filtrées.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les normes calculées.\n",
    "    \"\"\"\n",
    "    normes = {}\n",
    "    traite = set() # Créer un ensemble vide pour stocker les capteurs, les côtés et les mesures déjà traités\n",
    "    for key, value in labels_data_filtre.items():\n",
    "        parts = key.split('_') # Séparer l'étiquette en parties\n",
    "        sensor = parts[0] # Récupérer le capteur\n",
    "        side = parts[1] # Récupérer le côté\n",
    "        measure = parts[2] # Récupérer la mesure (GYRO ou ACC)\n",
    "        \n",
    "        if (sensor, side, measure) not in traite: # Si le capteur, le côté et la mesure n'ont pas déjà été traités\n",
    "            traite.add((sensor, side, measure)) # Ajouter le capteur, le côté et la mesure à l'ensemble des éléments traités\n",
    "            \n",
    "            if \"ACC\" in measure:\n",
    "                # Obtention des axes X, Y et Z\n",
    "                axe_X = (labels_data_filtre[f'{sensor}_{side}_{measure}_X'])\n",
    "                axe_Y = (labels_data_filtre[f'{sensor}_{side}_{measure}_Y'])\n",
    "                axe_Z = (labels_data_filtre[f'{sensor}_{side}_{measure}_Z'])\n",
    "            \n",
    "                norme = np.sqrt(axe_X**2 + axe_Y**2 + axe_Z**2) - 1 # Calcul de la norme auquelle on soustrait 1 pour enlever la gravité\n",
    "                nom_cle = f'{sensor}_{side}_{measure}_norme'\n",
    "                normes[nom_cle] = norme\n",
    "                \n",
    "            else:\n",
    "                axe_X = labels_data_filtre[f'{sensor}_{side}_{measure}_X']\n",
    "                axe_Y = labels_data_filtre[f'{sensor}_{side}_{measure}_Y']\n",
    "                axe_Z = labels_data_filtre[f'{sensor}_{side}_{measure}_Z']\n",
    "        \n",
    "                norme = np.sqrt(axe_X**2 + axe_Y**2 + axe_Z**2) \n",
    "                nom_cle = f'{sensor}_{side}_{measure}_norme'\n",
    "                normes[nom_cle] = norme\n",
    "\n",
    "    return normes\n",
    "\n",
    "def creer_structure_json(labels_data_filtre, patient_id, date_de_naissance, medicaments, resampled_times, events_dict, normes):\n",
    "    \"\"\"\n",
    "    Cette fonction crée une structure JSON à partir des données filtrées et d'autres informations.\n",
    "\n",
    "    Args:\n",
    "        labels_data_filtre (dict): Dictionnaire contenant les étiquettes et les données associées, filtrées.\n",
    "        patient_id (int): Identifiant du patient.\n",
    "        date_de_naissance (str): Date de naissance du patient.\n",
    "        medicaments (str): Médicaments pris par le patient.\n",
    "        resampled_times (ndarray): Temps rééchantillonné.\n",
    "        events_dict (dict): Dictionnaire contenant les événements et leur temps correspondant.\n",
    "        normes (dict): Dictionnaire contenant les normes calculées.\n",
    "\n",
    "    Returns:\n",
    "        dict: Structure JSON contenant les données et les métadonnées.\n",
    "    \"\"\"\n",
    "    json_data = {\n",
    "        \"metadata\": {\n",
    "            \"Details du patient\": {\n",
    "                \"Identifiant\": patient_id,\n",
    "                \"Date de naissance\": date_de_naissance,\n",
    "                \"Medicaments\": medicaments\n",
    "            },\n",
    "            \"Temps\": resampled_times.tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for key, value in labels_data_filtre.items():\n",
    "        parts = key.split('_')\n",
    "        sensor = parts[1]\n",
    "        side = parts[0]\n",
    "        measure = parts[2]\n",
    "        axis = parts[3]\n",
    "        \n",
    "        if sensor not in json_data:\n",
    "            json_data[sensor] = {}\n",
    "\n",
    "        if side not in json_data[sensor]:\n",
    "            json_data[sensor][side] = {}\n",
    "\n",
    "        if measure not in json_data[sensor][side]:\n",
    "            json_data[sensor][side][measure] = {}\n",
    "\n",
    "        json_data[sensor][side][measure][axis] = value.tolist()\n",
    "        \n",
    "    for key in normes:\n",
    "        parts = key.split('_')\n",
    "        sensor = parts[1]\n",
    "        side = parts[0]\n",
    "        measure = parts[2]\n",
    "        axis = parts[3]\n",
    "\n",
    "        if sensor not in json_data:\n",
    "            json_data[sensor] = {}\n",
    "\n",
    "        if side not in json_data[sensor]:\n",
    "            json_data[sensor][side] = {}\n",
    "\n",
    "        if measure not in json_data[sensor][side]:\n",
    "            json_data[sensor][side][measure] = {}\n",
    "\n",
    "        # Insérer la norme au même niveau d'indentation que l'axe\n",
    "        json_data[sensor][side][measure][axis] = value.tolist()\n",
    "        json_data[sensor][side][measure][\"norme\"] = normes[key].tolist()\n",
    "    \n",
    "        # # Ajouter les événements FOG\n",
    "        # json_data[\"FOG\"] = {\n",
    "        #     \"Debut\": events_dict[\"FOG_begin\"],\n",
    "        #     \"Fin\": events_dict[\"FOG_end\"]\n",
    "        # }\n",
    "    \n",
    "        # # Ajouter tous les évènements sauf FOG\n",
    "        # del events_dict[\"FOG_begin\"]\n",
    "        # del events_dict[\"FOG_end\"]\n",
    "            \n",
    "    if \"FOG_begin\" in events_dict and \"FOG_end\" in events_dict:\n",
    "        json_data[\"FOG\"] = {\n",
    "            \"Debut\": events_dict[\"FOG_begin\"], \n",
    "            \"Fin\": events_dict[\"FOG_end\"]\n",
    "        }\n",
    "        del events_dict[\"FOG_begin\"]\n",
    "        del events_dict[\"FOG_end\"]\n",
    "    else:\n",
    "        json_data[\"FOG\"] = {\n",
    "            \"Debut\": [0],\n",
    "            \"Fin\": [0]\n",
    "        }\n",
    "    \n",
    "    json_data[\"Parcours\"] = events_dict\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "def creation_json_grace_c3d(file_path, patient_id, date_de_naissance, medicaments, output_path):\n",
    "    \"\"\"\n",
    "    Cette fonction crée un fichier JSON à partir d'un fichier C3D.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Le chemin du fichier C3D.\n",
    "        patient_id (int): Identifiant du patient.\n",
    "        date_de_naissance (str): Date de naissance du patient.\n",
    "        medicaments (str): Médicaments pris par le patient.\n",
    "        output_path (str): Le chemin de sortie du fichier JSON.\n",
    "    \"\"\"\n",
    "    events_dict = recuperer_evenements(file_path) \n",
    "    filtered_fusion_label_data, resampled_times = reechantillonnage_fc_coupure_et_association_labels_et_data(file_path)\n",
    "    labels_filtre, labels_data_filtre = filtrer_labels(filtered_fusion_label_data)\n",
    "    normes = calcul_norme(labels_data_filtre)\n",
    "    json_structure = creer_structure_json(labels_data_filtre, patient_id, date_de_naissance, medicaments, resampled_times, events_dict, normes)\n",
    "    \n",
    "    with open(output_path, \"w\") as fichier_json:\n",
    "        json.dump(json_structure, fichier_json, indent=4)\n",
    "\n",
    "# Utilisation de la fonction pour traiter le fichier C3D et générer le fichier JSON\n",
    "fichier_brut = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/LE_LIEVRE_Emmanuel_1971_03_19_LEEM1971/2023-05-26/2023-05-26_overlay_detectFOG/Video overlay 16 - Anthony.c3d\"\n",
    "fichier_brut_6 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/MORETTY_Daniel_1963-06-23_MODA1963/2023-07-07/2023-07-07_overlay_unspecified/Video overlay 6.c3d\"\n",
    "fichier_brut_16 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/LE_LIEVRE_Emmanuel_1971_03_19_LEEM1971/2023-05-26/2023-05-26_overlay_detectFOG/Video overlay 6_start_stop.c3d\"\n",
    "fichier_brut_3 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/MORETTY_Daniel_1963-06-23_MODA1963/2023-07-07/2023-07-07_overlay_unspecified/Video overlay 3.c3d\"\n",
    "\n",
    "patient_id = 1234\n",
    "date_de_naissance = 45\n",
    "medicaments = \"ON\"\n",
    "chemin_fichier_json_6_start_stop = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 6_start_stop.json\"\n",
    "chemin__fichier_json = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 6.json\"\n",
    "chemin__fichier_json_3 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 3.json\"\n",
    "creation_json_grace_c3d(fichier_brut_16, patient_id, date_de_naissance, medicaments, chemin_fichier_json_6_start_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Très bizarre, malgré le fait qu'il n'y est aucun évènement de noté, le code me sort quand même des évènements. Cela est peut-être lié à des\n",
    "étiquettes par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_fichier_json_16 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 16.json\"\n",
    "chemin_fichier_json_anthony = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 16 - Anthony.json\"\n",
    "chemin_fichier_json_6_start_stop = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 6_start_stop.json\"\n",
    "chemin__fichier_json = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 6.json\"\n",
    "chemin__fichier_json_3 = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/Video overlay 3.json\"\n",
    "\n",
    "# Charger le fichier JSON\n",
    "with open(chemin_fichier_json_16, \"r\") as fichier_json:\n",
    "    donnees_patient_16 = json.load(fichier_json)\n",
    "    \n",
    "with open(chemin_fichier_json_anthony, \"r\") as fichier_json:\n",
    "    donnees_patient_anthony = json.load(fichier_json) \n",
    "    \n",
    "with open(chemin__fichier_json, \"r\") as fichier_json:\n",
    "    donnees_patient_1 = json.load(fichier_json)   \n",
    "\n",
    "with open(chemin_fichier_json_6_start_stop, \"r\") as fichier_json:\n",
    "    donnees_patient_16_update = json.load(fichier_json)   \n",
    "    \n",
    "with open(chemin__fichier_json_3, \"r\") as fichier_json:\n",
    "    donnees_patient_3 = json.load(fichier_json) \n",
    "\n",
    "# Accéder aux données spécifiques\n",
    "#temps = donnees_patient[\"Rectus Femoris\"][\"Left\"][\"ACC\"][\"norme\"]\n",
    "#temps_16 = donnees_patient_16[\"metadata\"][\"Temps\"]\n",
    "#Norme_GYRO_tibia_16 = donnees_patient_16[\"Tibialis Anterior\"][\"Left\"][\"GYRO\"][\"norme\"]\n",
    "#FOG_overlay_16 = donnees_patient_16[\"FOG\"]\n",
    "#parcours_16 = donnees_patient_16[\"Parcours\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_interval(data):\n",
    "    \"\"\"\n",
    "    Extrait un intervalle de données à partir du moment où le patient se lève jusqu'au dernier moment debout.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Un dictionnaire contenant les données des capteurs.\n",
    "\n",
    "    Returns:\n",
    "    - data_interval (dict): Un dictionnaire contenant l'intervalle de données extrait.\n",
    "    \"\"\"\n",
    "    # Extraire les temps de début et de fin du parcours\n",
    "    start_time = data[\"Parcours\"][\"START\"][0] # Extraire le temps de début du parcours\n",
    "    end_time = data[\"Parcours\"][\"END\"][0] # Extraire le temps de fin du parcours\n",
    "    epsilon=0.01 # Marge d'erreur\n",
    "\n",
    "    # Trouver les indices correspondants dans le vecteur de temps pour le début du parcours\n",
    "    for i, time in enumerate(data[\"metadata\"][\"Temps\"]): # Pour chaque temps dans le vecteur de temps\n",
    "        if abs(time - start_time) < epsilon:  # Vérifier si la différence est inférieure à la marge d'erreur\n",
    "            start_index = i \n",
    "        if abs(time - end_time) < epsilon:\n",
    "            end_index = i\n",
    "\n",
    "    # Extraire les données d'axes pour la plage de temps START à END\n",
    "    data_interval = {}\n",
    "    for sensor, sensor_data in data.items():\n",
    "        if sensor not in [\"metadata\", \"Parcours\", \"FOG\"]:\n",
    "            data_interval[sensor] = {}\n",
    "            for side, side_data in sensor_data.items():\n",
    "                data_interval[sensor][side] = {}\n",
    "                for measure, measure_data in side_data.items():\n",
    "                    data_interval[sensor][side][measure] = {}\n",
    "                    for axis, axis_data in measure_data.items():\n",
    "                        data_interval[sensor][side][measure][axis] = axis_data[start_index:end_index+1]\n",
    "\n",
    "    # Copier les données de \"metadata\", \"Parcours\" et \"FOG\"\n",
    "    data_interval[\"metadata\"] = data[\"metadata\"]\n",
    "    data_interval[\"Parcours\"] = data[\"Parcours\"]\n",
    "    data_interval[\"FOG\"] = data[\"FOG\"]\n",
    "\n",
    "    # Extraire la plage de temps START à END pour la liste de temps dans metadata\n",
    "    metadata_temps_interval = data[\"metadata\"][\"Temps\"][start_index:end_index+1]\n",
    "\n",
    "    # Ajouter la plage de temps interval à metadata\n",
    "    data_interval[\"metadata\"][\"Temps\"] = metadata_temps_interval\n",
    "\n",
    "    return data_interval\n",
    "\n",
    "# Utilisation de la fonction\n",
    "data_interval = extract_data_interval(donnees_patient_16_update)\n",
    "data_interval_16 = extract_data_interval(donnees_patient_16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On applique la formule de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Normalise les données des capteurs.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Un dictionnaire contenant les données des capteurs.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data (dict): Un dictionnaire contenant les données normalisées.\n",
    "    \"\"\"\n",
    "    normalized_data = {}\n",
    "    for sensor, sensor_data in data.items():\n",
    "        if sensor not in [\"metadata\", \"Parcours\", \"FOG\"]:\n",
    "            normalized_data[sensor] = {}\n",
    "            for side, side_data in sensor_data.items():\n",
    "                normalized_data[sensor][side] = {}\n",
    "                for measure, measure_data in side_data.items():\n",
    "                    normalized_data[sensor][side][measure] = {}\n",
    "                    for axis, axis_data in measure_data.items():\n",
    "                        # Calculer la moyenne, le maximum et le minimum des données\n",
    "                        mean = np.mean(axis_data)\n",
    "                        max_val = np.max(axis_data)\n",
    "                        min_val = np.min(axis_data)\n",
    "                        # Appliquer la normalisation sur tous les axes X,Y,Z et la norme\n",
    "                        normalized_axis_data = (axis_data - mean) / (max_val - min_val)\n",
    "                        normalized_data[sensor][side][measure][axis] = normalized_axis_data\n",
    "    \n",
    "    # Copier les données de \"metadata\", \"Parcours\" et \"FOG\"\n",
    "    normalized_data[\"metadata\"] = data[\"metadata\"]\n",
    "    normalized_data[\"Parcours\"] = data[\"Parcours\"]\n",
    "    normalized_data[\"FOG\"] = data[\"FOG\"]\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "normalized_data = normalize_data(data_interval)\n",
    "normalized_data_16 = normalize_data(data_interval_16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fenêtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoupage_en_fenetres(data, taille_fenetre, decalage, taux_echantillonnage):\n",
    "    fenetres_data = {}\n",
    "    infos_fenetres = {}\n",
    "\n",
    "    for sensor, sensor_data in data.items():\n",
    "        if sensor not in [\"metadata\", \"Parcours\", \"FOG\"]:\n",
    "            fenetres_data[sensor] = {}\n",
    "            infos_fenetres[sensor] = {}\n",
    "\n",
    "            for side, side_data in sensor_data.items():\n",
    "                fenetres_data[sensor][side] = {}\n",
    "                infos_fenetres[sensor][side] = {}\n",
    "\n",
    "                for measure, measure_data in side_data.items():\n",
    "                    fenetres_data[sensor][side][measure] = {}\n",
    "                    infos_fenetres[sensor][side][measure] = {}\n",
    "\n",
    "                    for axis, axis_data in measure_data.items():\n",
    "                        taille_signal = len(axis_data)\n",
    "                        taille_fenetre_echantillons = int(taille_fenetre * taux_echantillonnage)\n",
    "                        decalage_fenetre = int(decalage * taille_fenetre_echantillons)\n",
    "\n",
    "                        fenetres = []\n",
    "                        debut = 0\n",
    "                        fin = taille_fenetre_echantillons\n",
    "                        nb_fenetres = 0\n",
    "\n",
    "                        while fin <= taille_signal:\n",
    "                            fenetre = axis_data[debut:fin]\n",
    "                            fenetres.append(fenetre)\n",
    "\n",
    "                            debut = debut + decalage_fenetre\n",
    "                            fin = fin + decalage_fenetre\n",
    "                            nb_fenetres += 1\n",
    "\n",
    "                        if debut < taille_signal:\n",
    "                            fenetre = axis_data[debut:]\n",
    "                            fenetres.append(fenetre)\n",
    "\n",
    "                        fenetres_data[sensor][side][measure][axis] = fenetres\n",
    "                        infos_fenetres[sensor][side][measure][axis] = {\n",
    "                            \"nombre_fenetres\": nb_fenetres,\n",
    "                            \"taille_fenetre\": taille_fenetre_echantillons,\n",
    "                            \"decalage_fenetre\": decalage_fenetre\n",
    "                        }\n",
    "\n",
    "    # Traitement des nouvelles données de temps\n",
    "    temps = data[\"metadata\"][\"Temps\"]\n",
    "    taille_signal_temps = len(temps)\n",
    "    taille_fenetre_temps = int(taille_fenetre * taux_echantillonnage)\n",
    "    decalage_fenetre_temps = int(decalage * taille_fenetre_temps)\n",
    "\n",
    "    fenetres_temps = []\n",
    "    debut_temps = 0\n",
    "    fin_temps = taille_fenetre_temps\n",
    "\n",
    "    while fin_temps <= taille_signal_temps:\n",
    "        fenetre_temps = temps[debut_temps:fin_temps]\n",
    "        fenetres_temps.append(fenetre_temps)\n",
    "\n",
    "        debut_temps += decalage_fenetre_temps\n",
    "        fin_temps += decalage_fenetre_temps\n",
    "\n",
    "    if debut_temps < taille_signal_temps:\n",
    "        fenetre_temps = temps[debut_temps:]\n",
    "        fenetres_temps.append(fenetre_temps)\n",
    "\n",
    "    # Copie des données de \"metadata\", \"Parcours\" et \"FOG\"\n",
    "    fenetres_data[\"metadata\"] = data[\"metadata\"]\n",
    "    fenetres_data[\"Parcours\"] = data[\"Parcours\"]\n",
    "    fenetres_data[\"FOG\"] = data[\"FOG\"]\n",
    "    \n",
    "    # Remplacement des anciennes données de temps par les nouvelles\n",
    "    fenetres_data[\"metadata\"][\"Temps\"] = fenetres_temps\n",
    "\n",
    "    return fenetres_data, infos_fenetres\n",
    "\n",
    "fenetres_data, infos_fenetres = decoupage_en_fenetres(normalized_data, taille_fenetre = 2, decalage=0.2, taux_echantillonnage=50)\n",
    "fenetres_data_16, infos_fenetres_16 = decoupage_en_fenetres(normalized_data_16, taille_fenetre = 2, decalage=0.2, taux_echantillonnage=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_fenetre(data,temps_prefog = 3):\n",
    "    temps = data[\"metadata\"][\"Temps\"]\n",
    "    debuts_fog = data[\"FOG\"][\"Debut\"]\n",
    "    fins_fog = data[\"FOG\"][\"Fin\"]\n",
    "    debuts_prefog = [x-temps_prefog for x in debuts_fog]\n",
    "    status=\"NoFOG\"\n",
    "    \n",
    "    # on stock les données d'évènement dans un dataframe ordonner en fonction du temps\n",
    "    events=pd.DataFrame({'temps': debuts_fog + fins_fog + debuts_prefog, \n",
    "                    'events': [\"debut_fog\"]*len(debuts_fog) + [\"fin_fog\"]*len(fins_fog) + [\"preFog\"]*len(debuts_prefog)}).sort_values('temps').reset_index(drop=True)\n",
    "\n",
    "    # On récupère si il y a un évènement de FOG ou plusieurs de présent de la FOG\n",
    "    statuses = []\n",
    "    status = \"NoFog\"\n",
    "\n",
    "    for window in temps:\n",
    "        w_start = window[0] # premier terme de la fenêtre\n",
    "        w_end = window[-1] # dernier terme de la fenêtre\n",
    "        window_events = []  # Liste pour stocker les événements de la fenêtre\n",
    "        time=[]\n",
    "        time_pourcent = 1 \n",
    "        \n",
    "        for _, row in events.iterrows():\n",
    "            if row['temps'] >= w_start and row['temps'] <= w_end: #si le temps correspondant à l'évènement se trouve entre début et fin de la fenêtre\n",
    "                window_events.append(row['events'])  # Ajouter l'événement à la liste\n",
    "                if row['events']== \"fin_fog\": # si l'évènement est fin_fog\n",
    "                    time.append(row[\"temps\"])\n",
    "                                    \n",
    "        if len(window_events)==1 and \"fin_fog\" in window_events: #si on oa une liste avec uniquement fin_fog\n",
    "             time_array = np.arange(w_start,w_end,1/50)\n",
    "             time_pourcent = np.sum(time_array<=time)/100\n",
    "    \n",
    "        if not window_events:  # Si la liste est vide\n",
    "            window_events = [None]  # Remplir avec None\n",
    "    \n",
    "        # if status == \"NoFog\" and \"preFog\" in window_events:\n",
    "        #     status = \"transitionPreFog\"\n",
    "        \n",
    "        # elif status == \"transitionPreFog\" and None in window_events:\n",
    "        #     status = \"preFog\"\n",
    "            \n",
    "        if status == \"NoFog\" and \"debut_fog\" in window_events: # si la fenêtre contient debut_fog et son statu est NoFog\n",
    "            status = \"transitionFog\"\n",
    "            \n",
    "        elif status == \"transitionFog\" and None in window_events: #si le FOG est suffisement long pour ne pas rencontrer d'évènement après debut_Fog alors :\n",
    "            status = \"Fog\"\n",
    "        \n",
    "        elif \"debut_fog\" in window_events and \"fin_fog\" in window_events: #si il est petit alors la fenêtre peut comporter l'évènement de début et de fin\n",
    "            status = \"Fog\"\n",
    "            \n",
    "        elif status ==\"Fog\" and (\"debut_fog\" in window_events and \"fin_fog\" in window_events): # dans le cas où des FOG sont succints, donc c'est à dire quand la fenêtre comporte fin_fog et debut du fog suivant alors :  \n",
    "            status= \"transitionFog\"\n",
    "        \n",
    "        elif status == \"Fog\" and \"fin_fog\" in window_events and time_pourcent <= 0.5: # si on a un FOG inférieur à 50% de la longueur de fenêtre, alors : \n",
    "        # on s'en fiche de faire cette opération avant, car dans tous les cas on considère FOG lorsqu'il y a deux évènements dans la fenêtre et on prend pour cible transitionFog,donc que ce soit FOG ou transition ce sera dans cible. \n",
    "            status = \"transitionNoFog\"\n",
    "            \n",
    "        elif status == \"transitionNoFog\" and None in window_events:\n",
    "            status = \"NoFog\"\n",
    "        \n",
    "        statuses.append(status)  # Ajouter le statut à la liste des statuts\n",
    "        \n",
    "    # on associe les labels de fenêtre dans notre data\n",
    "    data[\"labels_fenetres\"] = statuses\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses_16=label_fenetre(fenetres_data_16)\n",
    "print(statuses_16[\"labels_fenetres\"])\n",
    "satuses=label_fenetre(fenetres_data)\n",
    "c= statuses_16[\"labels_fenetres\"]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintenant j'aimerais définir le label de chaque fenêtre en clé de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_label_fenetre_data(satuses):\n",
    "    association_label_fenetre_data = {}\n",
    "    for sensor, sensor_data in satuses.items():\n",
    "        if sensor not in [\"metadata\", \"Parcours\", \"FOG\", \"labels_fenetres\"]:\n",
    "            association_label_fenetre_data[sensor] = {}\n",
    "            for side, side_data in sensor_data.items():\n",
    "                association_label_fenetre_data[sensor][side] = {}\n",
    "\n",
    "                for measure, measure_data in side_data.items():\n",
    "                    association_label_fenetre_data[sensor][side][measure] = {}\n",
    "                    \n",
    "                    for axis, axis_data in measure_data.items():\n",
    "                        association_label_fenetre_data[sensor][side][measure][axis] = {}\n",
    "                        presentWin=np.unique(satuses[\"labels_fenetres\"])\n",
    "                        for label in presentWin:\n",
    "                            #data_list = axis_data\n",
    "                            data_frame_axis_data= pd.DataFrame(axis_data)\n",
    "                            association_label_fenetre_data[sensor][side][measure][axis][label]=data_frame_axis_data[[x==label for x in satuses[\"labels_fenetres\"]]]\n",
    "    \n",
    "        # Copie des données de \"metadata\", \"Parcours\" et \"FOG\"\n",
    "    association_label_fenetre_data[\"metadata\"] = satuses[\"metadata\"]\n",
    "    association_label_fenetre_data[\"Parcours\"] = satuses[\"Parcours\"]\n",
    "    association_label_fenetre_data[\"FOG\"] = satuses[\"FOG\"]\n",
    "\n",
    "    association_label_fenetre_data[\"FOG\"] = {\n",
    "        \"Debut\": satuses[\"FOG\"][\"Debut\"],\n",
    "        \"Fin\": satuses[\"FOG\"][\"Fin\"],\n",
    "        \"preFog\": [max(0, x - 3) for x in satuses[\"FOG\"][\"Debut\"]]\n",
    "    }\n",
    "    return association_label_fenetre_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=association_label_fenetre_data(satuses)\n",
    "temp_16=association_label_fenetre_data(statuses_16)\n",
    "print(temp[\"Tibialis Anterior\"][\"Left\"][\"GYRO\"][\"norme\"][\"NoFog\"])\n",
    "print(temp[\"FOG\"][\"Debut\"])\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_16[\"metadata\"][\"Temps\"] = pd.DataFrame(temp_16[\"metadata\"][\"Temps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = temp_16[\"metadata\"][\"Temps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time= temp_16[\"metadata\"][\"Temps\"].iloc[0,0]\n",
    "if np.isnan(temp_16[\"metadata\"][\"Temps\"].iloc[-1,-1]):\n",
    "    first_na = np.where(np.isnan(temp_16[\"metadata\"][\"Temps\"]))[-1][0]\n",
    "    last_time = temp_16[\"metadata\"][\"Temps\"].iloc[-1,(first_na-1)]\n",
    "else:\n",
    "    last_time = temp_16[\"metadata\"][\"Temps\"].iloc[-1,-1]\n",
    "\n",
    "print(last_time)\n",
    "\n",
    "\n",
    "temps_total = last_time - first_time\n",
    "print(\"Temps total de l'enregistrement :\", temps_total, \"secondes\")\n",
    "\n",
    "if  temp_16[\"FOG\"][\"Debut\"] == [0] :\n",
    "    nb_fog = 0\n",
    "    print(f\"Nombre de FOG : {nb_fog}\")\n",
    "else :\n",
    "    nb_fog = len(temp_16[\"FOG\"][\"Debut\"])\n",
    "    print(f\"Nombre de FOG : {nb_fog}\")\n",
    "\n",
    "temps_fog = sum([fin - debut for debut, fin in zip(temp_16[\"FOG\"][\"Debut\"], temp_16[\"FOG\"][\"Fin\"])])\n",
    "prct_fog = (temps_fog / temps_total) * 100\n",
    "print(f\"Pourcentage total de FOG sur la totalité de l'enregistrement : {prct_fog:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_test =temp_16[\"metadata\"][\"Temps\"]\n",
    "nb_fenetre = len(temp_16[\"metadata\"][\"Temps\"])\n",
    "nb_colonne = len(temp_16[\"metadata\"][\"Temps\"].columns)\n",
    "print(f\"Nombre de fenêtres : {nb_fenetre}\")\n",
    "print(f\"Nombre de colonnes : {nb_colonne}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_stat= pd.DataFrame({\"Temps total de l'enregistrement\": [temps_total], \"Nombre de FOG\": [nb_fog], \"Pourcentage total de FOG\": [prct_fog], \"nombre de fenêtres\": [nb_fenetre], \"longueur des fenêtres\": [nb_colonne]})\n",
    "print(tab_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temps_fog = [fin - debut for debut, fin in zip(temp_16[\"FOG\"][\"Debut\"], temp_16[\"FOG\"][\"Fin\"])]\n",
    "tab_fog = pd.DataFrame({\"Debut\": temp_16[\"FOG\"][\"Debut\"], \"Fin\": temp_16[\"FOG\"][\"Fin\"], \"Total\" : temps_fog})\n",
    "print(tab_fog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_16[\"metadata\"][\"Temps\"] = pd.DataFrame(temp_16[\"metadata\"][\"Temps\"])\n",
    "print(temp_16[\"metadata\"][\"Temps\"].iloc[0,0])\n",
    "# Trouver l'indice de la dernière valeur non nulle\n",
    "# Trouver la dernière valeur non nulle avant NaN dans la dernière colonne\n",
    "first_na=np.where(np.isnan(temp_16[\"metadata\"][\"Temps\"]))[-1][0]\n",
    "last_time=temp_16[\"metadata\"][\"Temps\"].iloc[-1,(first_na-1)]\n",
    "print(last_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
