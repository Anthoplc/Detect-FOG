{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection des FOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Chargement des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezc3d\n",
    "from pyomeca import Analogs, Markers\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtenez le répertoire imus6_subjects7\n",
    "repertoire_imus6_subjects7 = 'C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/data_article/raw/imus6_subjects7'\n",
    "\n",
    "# Chargez les données à partir du fichier Excel en utilisant le chemin relatif\n",
    "pt1_visit_24_tbc_walklr_0_trial_2 = pd.read_excel(os.path.join(repertoire_imus6_subjects7, 'pt1_visit_24_tbc_walklr_0_trial_2.xlsx'))\n",
    "pt1_visit_24_tbc_walklr_0_trial_4 = pd.read_excel(os.path.join(repertoire_imus6_subjects7, 'pt1_visit_24_tbc_walklr_0_trial_4.xlsx'))\n",
    "\n",
    "# Supprimez les colonnes contenant des valeurs manquantes pour avoir les colonnes avec les IMUs actifs\n",
    "pt1_visit_24_tbc_walklr_0_trial_2.dropna(axis=1, inplace=True)\n",
    "pt1_visit_24_tbc_walklr_0_trial_4.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_brut = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/LE_LIEVRE_Emmanuel_1971_03_19_LEEM1971/2023-05-26/2023-05-26_overlay_detectFOG/Video_overlay_4.c3d\"\n",
    "c3d = ezc3d.c3d(fichier_brut)\n",
    "\n",
    "labels = c3d['parameters']['ANALOG']['LABELS']['value']\n",
    "frequence = c3d['parameters']['ANALOG']['RATE']['value'][0]\n",
    "data = c3d['data']['analogs']\n",
    "\n",
    "def associer_labels_et_data(labels,data):\n",
    "    fusion_label_data = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        fusion_label_data[label] = data[:, i]\n",
    "    return fusion_label_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fusion_label_data = associer_labels_et_data(labels, data)\n",
    "for label, values in fusion_label_data.items(): \n",
    "    parts = label.split('_')\n",
    "    capteur_nom = parts[1]\n",
    "    lateralite = parts[0]\n",
    "    mesure_type = parts[1]\n",
    "    axes = parts[-1] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction json final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ezc3d\n",
    "import json\n",
    "def filtrer_signaux_utils(c3d):\n",
    "    labels = c3d['parameters']['ANALOG']['LABELS']['value']\n",
    "    frequence = c3d['parameters']['ANALOG']['RATE']['value'][0]\n",
    "    data = c3d['data']['analogs']\n",
    "    indices = []\n",
    "    labels_filtre = []\n",
    "    for i, label in enumerate(labels):\n",
    "        if 'ACC' in label or 'GYRO' in label:\n",
    "            indices.append(i)\n",
    "            labels_filtre.append(label)\n",
    "    \n",
    "    data_filtre = data[:, indices]\n",
    "    data_filtre = data_filtre[0]\n",
    "    return labels_filtre, data_filtre\n",
    "\n",
    "def associer_labels_et_data(labels,data):\n",
    "    fusion_label_data = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        fusion_label_data[label] = data[:, i]\n",
    "    return fusion_label_data\n",
    "\n",
    "def creer_structure_json(labels, fusion_label_data, patient_id, date_de_naissance, medicaments):\n",
    "    json_data = {\n",
    "        \"metadata\": {\n",
    "            \"Détails du patient\": {\n",
    "                \"Identifiant\": patient_id,\n",
    "                \"Date de naissance\": date_de_naissance,\n",
    "                \"Medicaments\": medicaments\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    capteur_types = set()\n",
    "    lateralite_types = set()\n",
    "    mesure_types = set()\n",
    "    axes_types = set()\n",
    "    \n",
    "    # Collecter les différentes étiquettes de la structure\n",
    "    for label in labels:\n",
    "        parts = label.split('_')\n",
    "        lateralite_types.add(parts[0])\n",
    "        capteur_types.add(parts[1])\n",
    "        mesure_types.add(parts[2])\n",
    "        axes_types.add(parts[-1])\n",
    "    \n",
    "    # Créer la structure de données\n",
    "    for capteur in capteur_types:\n",
    "        capteur_label = {}\n",
    "        for lateralite in lateralite_types:\n",
    "            lateralite_label = {}\n",
    "            for mesure_type in mesure_types:\n",
    "                mesure_label = {}\n",
    "                for axes in axes_types:\n",
    "                    mesure_label[axes] = []\n",
    "                lateralite_label[mesure_type] = mesure_label\n",
    "            capteur_label[lateralite] = lateralite_label\n",
    "        json_data[capteur] = capteur_label\n",
    "    \n",
    "    # Remplir la structure de données\n",
    "    for label, values in fusion_label_data.items():\n",
    "        parts = label.split('_')\n",
    "        capteur_nom = parts[1]\n",
    "        lateralite = parts[0]\n",
    "        mesure_type = parts[2]\n",
    "        axes = parts[-1]\n",
    "        \n",
    "        json_data[capteur_nom][lateralite][mesure_type][axes] = values.tolist()\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "def creation_json_grace_c3d(file_path, patient_id, date_de_naissance, medicaments, output_path):\n",
    "    c3d = ezc3d.c3d(file_path)\n",
    "    labels, data = filtrer_signaux_utils(c3d)\n",
    "    fusion_label_data = associer_labels_et_data(labels, data)\n",
    "    \n",
    "    json_structure = creer_structure_json(labels, fusion_label_data, patient_id, date_de_naissance, medicaments)\n",
    "    \n",
    "    with open(output_path, \"w\") as fichier_json:\n",
    "        json.dump(json_structure, fichier_json, indent=4)\n",
    "\n",
    "# Utilisation de la fonction pour traiter le fichier C3D et générer le fichier JSON\n",
    "fichier_brut = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATA_FOG/LE_LIEVRE_Emmanuel_1971_03_19_LEEM1971/2023-05-26/2023-05-26_overlay_detectFOG/Video_overlay_4.c3d\"\n",
    "patient_id = 1234\n",
    "date_de_naissance = 45\n",
    "medicaments = \"ON\"\n",
    "chemin_sortie = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/video_overlay_4.json\"\n",
    "\n",
    "creation_json_grace_c3d(fichier_brut, patient_id, date_de_naissance, medicaments, chemin_sortie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données de l'accéléromètre gauche sur l'axe Z du Rectus Femoris:\n",
      "[0.0126953125, -1.001953125, 0.02587890625, 22.7439022064209, 11.463415145874023, 0.060975611209869385, -0.13427734375, -1.00634765625, -0.03662109375, 18.35365867614746, 13.841464042663574, -1.097561001777649, 0.9150390625, -0.20361328125, 0.2939453125, -8.536585807800293, 19.268293380737305, -2.012195110321045, 0.09033203125, -1.00439453125, 0.09130859375, 20.365854263305664, 7.621951580047607, 2.682926893234253, 0.19189453125, -0.9809570908546448, -0.2153320163488388, -3.2926828861236572, -4.695122241973877, -0.7317073345184326, -0.92333984375, -0.30078125, 0.189453125, 5.609755992889404, 0.060975611209869385, 3.841463565826416, 0.006835937965661287, -1.0009765625, -0.25390625, 7.5, 20.121952056884766, -2.012195110321045]\n"
     ]
    }
   ],
   "source": [
    "# Charger le fichier JSON\n",
    "chemin_fichier_json = \"C:/Users/antho/Documents/MEMOIRE_M2/CODE_STAGE_M2/DATABASE/video_overlay_4.json\"\n",
    "with open(chemin_fichier_json, \"r\") as fichier_json:\n",
    "    donnees_patient = json.load(fichier_json)\n",
    "\n",
    "# Accéder aux données spécifiques\n",
    "accelerometre_right_Z_rectus_femoris = donnees_patient[\"Rectus Femoris\"][\"Left\"][\"GYRO\"][\"X\"]\n",
    "\n",
    "# Afficher les données\n",
    "print(\"Données de l'accéléromètre gauche sur l'axe Z du Rectus Femoris:\")\n",
    "print(accelerometre_right_Z_rectus_femoris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Filtrage des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from scipy.signal import butter, filtfilt\n",
    "# Fonction pour concevoir le filtre Butterworth\n",
    "def butterworth_filter(data, cutoff_frequency, sampling_rate, order=8):\n",
    "    nyquist_frequency = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_frequency / nyquist_frequency\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Fréquence de coupure et taux d'échantillonnage\n",
    "cutoff_frequency = 20  # Hz\n",
    "sampling_rate = 1 / (pt1_visit_24_tbc_walklr_0_trial_2['time'].iloc[1] - pt1_visit_24_tbc_walklr_0_trial_2['time'].iloc[0])  # Calcul du taux d'échantillonnage\n",
    "\n",
    "# Colonnes à exclure du filtre\n",
    "columns_to_exclude = ['time', 'subject_ID', 'freeze_label']\n",
    "\n",
    "# Colonnes à filtrer\n",
    "columns_to_filter = [col for col in pt1_visit_24_tbc_walklr_0_trial_2.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Appliquer le filtre à toutes les colonnes sauf celles exclues\n",
    "for column in columns_to_filter:\n",
    "    filtered_data = butterworth_filter(pt1_visit_24_tbc_walklr_0_trial_2[column], cutoff_frequency, sampling_rate)\n",
    "    pt1_visit_24_tbc_walklr_0_trial_2[f'filtered_{column}'] = filtered_data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Fenêtrage de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decouper_en_fenetres(signal, taille_fenetre, decalage):\n",
    "    \"\"\"\n",
    "    Découper le signal en fenêtres temporelles avec un chevauchement de 80%, autrement dit un décalage de 20% entre chaque fenêtre.\n",
    "\n",
    "    \"\"\"\n",
    "    taille_signal = len(signal)\n",
    "    taille_fenetre_echantillons = int(taille_fenetre * taux_echantillonnage)\n",
    "    decalage_fenetre = int(decalage * taille_fenetre_echantillons)\n",
    "\n",
    "    fenetres = []\n",
    "\n",
    "    debut = 0\n",
    "    fin = taille_fenetre_echantillons\n",
    "\n",
    "    while fin <= taille_signal:\n",
    "        fenetre = signal[debut:fin]\n",
    "        fenetres.append(fenetre)\n",
    "\n",
    "        # Mise à jour des indices pour la prochaine fenêtre\n",
    "        debut = debut + decalage_fenetre\n",
    "        fin = fin + decalage_fenetre\n",
    "\n",
    "    return fenetres\n",
    "\n",
    "# Utilisation de la fonction pour découper le signal en fenêtres\n",
    "taux_echantillonnage = 148 # en Hz\n",
    "taille_fenetre = 3  # en secondes\n",
    "decalage = 0.2  # en %, soit 0.6s\n",
    "\n",
    "\n",
    "fenetres = decouper_en_fenetres(pt1_visit_24_tbc_walklr_0_trial_2, taille_fenetre, decalage)\n",
    "    \n",
    "print(\"Nombre de fenêtres:\", len(fenetres))\n",
    "print(\"Taille de chaque fenêtre:\", len(fenetres[0]), \"frames\")\n",
    "print(\"Taille du décalage: \", int(decalage * (taille_fenetre * taux_echantillonnage)),\" frames\")\n",
    "\n",
    "# Pour vérifier si toutes les fenêtres respectent le bon format\n",
    "for i, fenetre in enumerate(fenetres[0:10]):\n",
    "    print(\"Taille de la fenêtre\",i+1,\":\", len(fenetres[i+1]), \"frames\")\n",
    "    \n",
    "'''Attention : Il manque 82 frames dans nos données finales, car nous avons un décalage inférieur à 88 frames pour générer une dernière fenêtre.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Visualisation graphique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "end_time = 1000\n",
    "\n",
    "def detect_and_plot_fog_periods(data, start_time, end_time, signals, title):\n",
    "    # Trouver les indices où le label change de 0 à 1 (début du FOG)\n",
    "    start_indices = np.where(np.diff(data['freeze_label']) == 1)[0] + 1\n",
    "\n",
    "    # Trouver les indices où le label change de 1 à 0 (fin du FOG)\n",
    "    end_indices = np.where(np.diff(data['freeze_label']) == -1)[0] + 1\n",
    "\n",
    "    # Filtrer les données en fonction des temps spécifiés\n",
    "    data_time_filtered = data[(data['time'] >= start_time) & (data['time'] <= end_time)]\n",
    "\n",
    "    # Créer des sous-graphiques pour chaque axe\n",
    "    fig, axs = plt.subplots(len(signals), figsize=(12, 6 * len(signals)), sharex=True)\n",
    "\n",
    "    colors = ['royalblue', 'forestgreen', 'darkorange']\n",
    "\n",
    "    for i, (signal, color) in enumerate(zip(signals, colors)):\n",
    "        # Tracer les signaux inertiométriques sur chaque sous-graphique\n",
    "        axs[i].plot(data_time_filtered['time'], data_time_filtered[signal], label=\"Signal\", color=color)\n",
    "\n",
    "        # Ajouter des lignes verticales pour marquer le début et la fin de chaque période de FOG\n",
    "        for start, end in zip(start_indices, end_indices):\n",
    "            axs[i].axvline(data_time_filtered['time'].iloc[start], color='r', linestyle='--', label='Début de FOG')\n",
    "            axs[i].axvline(data_time_filtered['time'].iloc[end], color='g', linestyle='--', label='Fin de FOG')\n",
    "        \n",
    "        # Colorier la zone entre les lignes verticales\n",
    "            axs[i].fill_between(data_time_filtered['time'].iloc[start:end + 1],\n",
    "                0, 1, color='gray', alpha=0.5, transform=axs[i].get_xaxis_transform())\n",
    "        \n",
    "        # Paramètres des sous-graphiques\n",
    "        axs[i].set(ylabel='Accélération (m/s)')\n",
    "        axs[i].set_title(f'{signal}')\n",
    "        \n",
    "        # Créer une légende pour chaque sous-graphique\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        axs[i].legend(by_label.values(), by_label.keys(), loc='upper left')\n",
    "\n",
    "    # Paramètres communs à tous les sous-graphiques\n",
    "    fig.suptitle(title)\n",
    "    axs[-1].set(xlabel='Temps')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Application de la fonction\n",
    "\n",
    "#Tracer les signaux provenant de l'accéléromètre du pied droit\n",
    "signals_acc_lumbar_to_plot = ['imu_foot_r_ax', 'imu_foot_r_ay', 'imu_foot_r_az']\n",
    "detect_and_plot_fog_periods(pt1_visit_24_tbc_walklr_0_trial_4, start_time, end_time, signals_acc_lumbar_to_plot, 'Détection des Périodes de FOG des signaux provenant de l\\'accéléromètre du pied droit')\n",
    "\n",
    "#Tracer les signaux provenant du gyroscope du pied droit\n",
    "signals_gyr_lumbar_to_plot = ['imu_foot_r_gx', 'imu_foot_r_gy', 'imu_foot_r_gz']\n",
    "detect_and_plot_fog_periods(pt1_visit_24_tbc_walklr_0_trial_4, start_time, end_time, signals_gyr_lumbar_to_plot, 'Détection des Périodes de FOG des signaux provenant du gyroscope du pied droit')\n",
    "\n",
    "#Tracer les signaux provenant de l'accéléromètre du pied gauche\n",
    "signals_acc_lumbar_to_plot = ['imu_foot_l_ax', 'imu_foot_l_ay', 'imu_foot_l_az']\n",
    "detect_and_plot_fog_periods(pt1_visit_24_tbc_walklr_0_trial_4, start_time, end_time, signals_acc_lumbar_to_plot, 'Détection des Périodes de FOG des signaux provenant de l\\'accéléromètre du pied droit')\n",
    "\n",
    "#Tracer les signaux provenant du gyroscope du pied gauche\n",
    "signals_gyr_lumbar_to_plot = ['imu_foot_l_gx', 'imu_foot_l_gy', 'imu_foot_l_gz']\n",
    "detect_and_plot_fog_periods(pt1_visit_24_tbc_walklr_0_trial_4, start_time, end_time, signals_gyr_lumbar_to_plot, 'Détection des Périodes de FOG des signaux provenant du gyroscope du pied droit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Extraction des caractéristiques du signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indice de gel (Moore et al, 2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "\n",
    "'''\n",
    "# Fonction pour calculer l'Indice de Gel (Freezing Index)\n",
    "def calculate_freezing_index(signal, fs):\n",
    "    # Paramètres de la méthode de Moore et al. (2008)\n",
    "    fb_band = (3, 8)  # Bande de gel de la marche en Hz\n",
    "    lb_band = (0.5, 3)  # Bande de locomotion en Hz\n",
    "    window_size = int(6 * fs)  # Fenêtre temporelle de 6 secondes\n",
    "\n",
    "    # Découper le signal en segments de 6 secondes\n",
    "    num_segments = len(signal) // window_size\n",
    "    segments = np.array_split(signal, num_segments)\n",
    "\n",
    "    freezing_index_values = []\n",
    "\n",
    "    for segment in segments:\n",
    "        # Calcul de la densité spectrale de puissance (PSD) avec la méthode de Welch\n",
    "        f, psd = welch(segment, fs=fs, nperseg=window_size)\n",
    "\n",
    "        # Extraire les indices de fréquence pour les bandes de gel et de locomotion\n",
    "        fb_indices = np.where((f >= fb_band[0]) & (f <= fb_band[1]))[0]\n",
    "        lb_indices = np.where((f >= lb_band[0]) & (f <= lb_band[1]))[0]\n",
    "\n",
    "        # Calculer l'Indice de Gel (Freezing Index)\n",
    "        freezing_index = np.sum(psd[fb_indices]) / np.sum(psd[lb_indices])\n",
    "        freezing_index_values.append(freezing_index)\n",
    "\n",
    "    return np.array(freezing_index_values)\n",
    "\n",
    "#Application de la fonction\n",
    "fs = 50  # Fréquence d'échantillonnage en Hz\n",
    "time = np.arange(0, len(pt1_visit_24_tbc_walklr_0_trial_2['imu_lumbar_ax']), 1) / fs\n",
    "acceleration_signal = pt1_visit_24_tbc_walklr_0_trial_2['imu_lumbar_ax']\n",
    "\n",
    "freezing_index_values = calculate_freezing_index(acceleration_signal, fs)\n",
    "\n",
    "# Tracer l'Indice de Gel sur le temps\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time[:len(freezing_index_values)], freezing_index_values, label='Indice de Gel (FI)')\n",
    "plt.xlabel('Temps (s)')\n",
    "plt.ylabel('Indice de Gel (FI)')\n",
    "plt.title('Analyse fréquentielle des signaux d\\'accéléromètre - Méthode de Moore et al. (2008)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show() '''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAGE_M2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
